from abc import ABC, abstractmethod
import logging
import requests
from typing import Optional, Dict, Type

# Registry to hold all available LLM providers
class LLMProviderRegistry:
    _providers: Dict[str, Type['LLMProvider']] = {}

    @classmethod
    def register_provider(cls, name: str, provider_cls: Type['LLMProvider']):
        if name in cls._providers:
            logging.warning(f"Provider '{name}' is already registered. Overwriting.")
        cls._providers[name] = provider_cls
        logging.info(f"Registered LLM provider: {name}")

    @classmethod
    def get_provider(cls, name: str) -> Type['LLMProvider']:
        provider = cls._providers.get(name)
        if not provider:
            logging.error(f"LLM provider '{name}' not found in registry.")
            raise ValueError(f"LLM provider '{name}' is not supported.")
        return provider

    @classmethod
    def list_providers(cls):
        return list(cls._providers.keys())

class LLMProvider(ABC):
    @abstractmethod
    def generate_response(self, prompt: str) -> str:
        pass

    @abstractmethod
    def embed_text(self, text: str) -> Optional[list]:
        """
        Generates an embedding for the given text.

        Args:
            text (str): The input text to embed.

        Returns:
            Optional[list]: The embedding vector if available, else None.
        """
        pass

# OllamaProvider Implementation
class OllamaProvider(LLMProvider):
    def __init__(self, api_url: str = "http://localhost:11434", model: str = "default", timeout: int = 30):
        """
        Initializes the OllamaProvider with the specified API URL and model.

        Args:
            api_url (str): The URL of the Ollama API.
            model (str): The name of the Ollama model to use.
            timeout (int): Timeout for API requests in seconds.
        """
        self.api_url = api_url
        self.model = model
        self.timeout = timeout
        logging.info(f"Initialized OllamaProvider with model '{self.model}' at API URL '{self.api_url}'.")

    def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the Ollama model based on the given prompt.

        Args:
            prompt (str): The input prompt for the LLM.

        Returns:
            str: The generated response from the model.
        """
        endpoint = f"{self.api_url}/generate"
        headers = {
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_tokens": 150,
            "temperature": 0.7
        }

        try:
            response = requests.post(endpoint, json=payload, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            data = response.json()
            generated_text = data.get("response", "").strip()
            logging.info("Received response from OllamaProvider.")
            return generated_text if generated_text else "No response generated by Ollama."
        except requests.exceptions.RequestException as e:
            logging.error(f"Error communicating with Ollama API: {e}")
            return "An error occurred while generating the response with Ollama."

    def embed_text(self, text: str) -> Optional[list]:
        """
        Generates an embedding for the provided text using the Ollama API.

        Args:
            text (str): The text to embed.

        Returns:
            Optional[list]: The embedding vector if successful, else None.
        """
        endpoint = f"{self.api_url}/embeddings"
        headers = {
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "prompt": text,
            "max_tokens": 150,
            "temperature": 0.7
        }

        try:
            response = requests.post(endpoint, json=payload, headers=headers, timeout=self.timeout)
            response.raise_for_status()
            data = response.json()
            embedding = data.get("embedding", None)
            if embedding:
                logging.info("Received embedding from OllamaProvider.")
                return embedding
            else:
                logging.warning("No embedding received from OllamaProvider.")
                return None
        except requests.exceptions.RequestException as e:
            logging.error(f"Error obtaining embedding from Ollama API: {e}")
            return None

# OpenAIProvider Implementation
class OpenAIProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "text-davinci-003"):
        """
        Initializes the OpenAIProvider with the provided API key and model.

        Args:
            api_key (str): API key for OpenAI.
            model (str): The name of the OpenAI model to use.
        """
        self.api_key = api_key
        self.model = model
        logging.info("Initialized OpenAIProvider.")

    def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the OpenAI model based on the given prompt.

        Args:
            prompt (str): The input prompt for the LLM.

        Returns:
            str: The generated response from the model.
        """
        import openai
        openai.api_key = self.api_key
        try:
            response = openai.Completion.create(
                engine=self.model,
                prompt=prompt,
                max_tokens=150,
                temperature=0.7
            )
            generated_text = response.choices[0].text.strip()
            logging.info("Received response from OpenAIProvider.")
            return generated_text if generated_text else "No response generated by OpenAI."
        except Exception as e:
            logging.error(f"OpenAI API error: {e}")
            return "An error occurred while generating the response with OpenAI."

    def embed_text(self, text: str) -> Optional[list]:
        """
        Generates an embedding for the provided text using the OpenAI API.

        Args:
            text (str): The text to embed.

        Returns:
            Optional[list]: The embedding vector if successful, else None.
        """
        import openai
        openai.api_key = self.api_key
        try:
            response = openai.Embedding.create(
                input=text,
                model="text-embedding-ada-002"
            )
            embedding = response['data'][0]['embedding']
            logging.info("Received embedding from OpenAIProvider.")
            return embedding
        except Exception as e:
            logging.error(f"OpenAI Embedding API error: {e}")
            return None

# AnthropicProvider Implementation
class AnthropicProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "claude-v1.3"):
        """
        Initializes the AnthropicProvider with the provided API key and model.

        Args:
            api_key (str): API key for Anthropic.
            model (str): The name of the Anthropic model to use.
        """
        self.api_key = api_key
        self.model = model
        logging.info("Initialized AnthropicProvider.")

    def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the Anthropic model based on the given prompt.

        Args:
            prompt (str): The input prompt for the LLM.

        Returns:
            str: The generated response from the model.
        """
        import anthropic
        client = anthropic.Anthropic(api_key=self.api_key)
        try:
            response = client.completion(
                model=self.model,
                prompt=anthropic.HUMAN_PROMPT + prompt + anthropic.AI_PROMPT,
                max_tokens_to_sample=150,
                temperature=0.7
            )
            generated_text = response['completion'].strip()
            logging.info("Received response from AnthropicProvider.")
            return generated_text if generated_text else "No response generated by Anthropic."
        except Exception as e:
            logging.error(f"Anthropic API error: {e}")
            return "An error occurred while generating the response with Anthropic."

    def embed_text(self, text: str) -> Optional[list]:
        """
        Generates an embedding for the provided text using the Anthropic API.

        Args:
            text (str): The text to embed.

        Returns:
            Optional[list]: The embedding vector if successful, else None.
        """
        # Assuming Anthropic provides an embedding API similar to OpenAI.
        # Since as of my knowledge cutoff in September 2021, Anthropic did not provide embeddings,
        # this is a placeholder implementation.
        logging.warning("AnthropicProvider does not support embeddings yet.")
        return None

# GoogleProvider Implementation
class GoogleProvider(LLMProvider):
    def __init__(self, credentials_path: str, model: str = "text-bison-001"):
        """
        Initializes the GoogleProvider with the provided credentials and model.

        Args:
            credentials_path (str): Path to the Google Cloud credentials JSON file.
            model (str): The name of the Google model to use.
        """
        import google.auth
        from google.cloud import aiplatform
        self.model = model
        try:
            credentials, project = google.auth.load_credentials_from_file(credentials_path)
            self.client = aiplatform.gapic.PredictionServiceClient(credentials=credentials)
            self.project = project
            self.location = "us-central1"
            self.endpoint = f"projects/{self.project}/locations/{self.location}/endpoints/{self.model}"
            logging.info("Initialized GoogleProvider.")
        except Exception as e:
            logging.error(f"Error initializing GoogleProvider: {e}")
            raise

    def generate_response(self, prompt: str) -> str:
        """
        Generates a response from the Google model based on the given prompt.

        Args:
            prompt (str): The input prompt for the LLM.

        Returns:
            str: The generated response from the model.
        """
        try:
            instance = {"content": prompt}
            response = self.client.predict(
                endpoint=self.endpoint,
                instances=[instance],
                parameters={"temperature": 0.7, "max_output_tokens": 150},
            )
            generated_text = response.predictions[0].get("content", "").strip()
            logging.info("Received response from GoogleProvider.")
            return generated_text if generated_text else "No response generated by Google."
        except Exception as e:
            logging.error(f"Google API error: {e}")
            return "An error occurred while generating the response with Google."

    def embed_text(self, text: str) -> Optional[list]:
        """
        Generates an embedding for the provided text using the Google API.

        Args:
            text (str): The text to embed.

        Returns:
            Optional[list]: The embedding vector if successful, else None.
        """
        try:
            # Assuming Google provides an embedding endpoint similar to their language models
            # This is a placeholder implementation.
            instance = {"content": text}
            response = self.client.predict(
                endpoint=self.endpoint,
                instances=[instance],
                parameters={"temperature": 0.0, "max_output_tokens": 0},
            )
            embedding = response.predictions[0].get("embedding", None)
            if embedding:
                logging.info("Received embedding from GoogleProvider.")
                return embedding
            else:
                logging.warning("No embedding received from GoogleProvider.")
                return None
        except Exception as e:
            logging.error(f"Google Embedding API error: {e}")
            return None

# Register Providers
LLMProviderRegistry.register_provider('ollama', OllamaProvider)
LLMProviderRegistry.register_provider('openai', OpenAIProvider)
LLMProviderRegistry.register_provider('anthropic', AnthropicProvider)
LLMProviderRegistry.register_provider('google', GoogleProvider)

# Example of Adding a Custom Provider
# Users can create separate modules for their providers and ensure they register themselves.
# Example:
# class CustomProvider(LLMProvider):
#     def __init__(self, ...):
#         ...
#     def generate_response(self, prompt: str) -> str:
#         ...
#     def embed_text(self, text: str) -> Optional[list]:
#         ...
# LLMProviderRegistry.register_provider('custom', CustomProvider)
